\section{Performance Analysis}
First, we begin with a performance analysis of EMBERS, from both a quantitative
point of view with respect to the GSR and with respect to end-user (analyst) goals.
\subsection{Quantitative Metrics}
\begin{figure}
%\includegraphics[width=0.8\columnwidth]{figures/cu/performance_tb1}

\definecolor{LightCyan}{rgb}{0.85,.95,.95}
\definecolor{yes}{RGB}{26,175,84}
\definecolor{no}{RGB}{253,191,45}
\definecolor{close}{RGB}{148,206,88}

\resizebox{\columnwidth}{!}{
\rowcolors{5}{gray!20}{white}
\begin{tabular}{|l|c|c|c|}
    \multicolumn{4}{c}{\textbf{Targets}} \\
    \hline \rowcolor{LightCyan}
                           & \textbf{Month 12}  & \textbf{Month 24} & \textbf{Month 36} \\
    \rowcolor{LightCyan}
                           & 4 months of
                           & 12 months of
                           &12 months of \\
    \rowcolor{LightCyan}
    \textbf{Metric}
                           & warnings
                           & warnings
                           & warnings \\
    \hline
    Mean Lead-Time         & 1 day    & 3 days   & 7 days \\
    \hline
    Mean Probability Score & 0.60     & 0.70     & 0.85 \\
    \hline
    Mean Quality Score     & 3.0      & 3.25     & 3.5 \\
    \hline
    Recall                 & 0.50     & 0.65     & 0.80 \\
    \hline
    Precision              & 0.50     & 0. 65    & 0.80 \\
    \hline
\end{tabular}
}
\resizebox{\columnwidth}{!}{
\rowcolors{2}{gray!20}{white}
\begin{tabular}{|l|c|c|c|}
    \multicolumn{4}{c}{\textbf{Actual}} \\
    \hline \rowcolor{LightCyan}
    \textbf{Metric}                 & \textbf{Month 12}  & \textbf{Month 24} & \textbf{Month 36} \\
    \hline
    Mean Lead-Time         & \cellcolor{yes} 3.89 days & \cellcolor{yes} 7.54 days & \cellcolor{yes} 9.76 days \\
    \hline
    Mean Probability Score & \cellcolor{yes} 0.72      & \cellcolor{yes} 0.89      & \cellcolor{yes} 0.88 \\
    \hline
    Mean Quality Score     & \cellcolor{no} 2.57       & \cellcolor{no}3.1         & \cellcolor{close}3.4 \\
    \hline
    Recall                 & \cellcolor{yes} 0.80      & \cellcolor{close} 0.65    & \cellcolor{yes} 0.79 \\
    \hline
    Precision              & \cellcolor{yes} 0.59      & \cellcolor{yes} 0.94      & \cellcolor{yes} 0.87 \\
    \hline
\end{tabular}
}
\caption{IARPA OSI targets and results achieved by EMBERS}
\label{fig:quant}
\end{figure}
Figure~\ref{fig:quant} depicts both the targets set by the IARPA OSI program as well as the
actual measures achieved by the EMBERS system. As shown here, the easiest target to achieve
in EMBERS was, surprisingly, the lead time objective. This was feasible due to EMBERS's focus on modeling
both planned and spontaneous events. Planned events are sometimes organized with as many as several weeks
of lead time and thus identifying indicators of organization was instrumental in achieving
lead time objectives. The confidence (mean probability) scores were also achieved by EMBERS and involved
careful calibration of probabilities by taking into account estimates of
model propensities and data source reliabilities. The measure that was most difficult to achieve
was the quality score as it involved a four component additive score and thus tangible improvements in
score required more than incremental improvements in forecasting specific components. Finally, recall
and precision involve a natural underlying trade-off and the deployment of our fusion/suppression
engine provided the ability to balance this trade-off to meet IARPA OSI's objectives.

Apart from comparing mean scores another interesting metric is to see
how many perfect matches (4.0 quality score) are obtained by a
algorithm.Figure~\ref{fig:perfect_score} shows the number of alerts issued
by EMBERS that matched perfectly to an event in the future on a monthly
basis for 2013.  The figure clearly shows that EMBERS makes almost double
the number of fully accurate forecasts as compared to the baserate
model.

% SATHAPPAN INSERT the quant table here.

\subsection{Analyst Evaluation}
In addition to the quantitative measures above, our experience interacting with analysts (across multiple
branches of government) demonstrated
an interesting dichotomy as to how analysts use EMBERS alerts. Some analysts preferred to use EMBERS in an
`analytic triage' scenario wherein they could tune EMBERS for high recall so that they would apply their
traditional measures of filtering and analysis to hone in on forecasts of interest. Other analysts
instead viewed EMBERS as a data source and preferred to use it in a high precision mode, e.g., wherein they
were focused on a specific region of the world (e.g., Venezuela) and aimed to investigate a particular
social science hypothesis (e.g., whether disruptions in global oil markets led to civil unrest).
To support this dichotomy of users, we implemented a mechanism wherein in addition to generating alerts, EMBERS
also forecasted the expected quality score for each forecast (using machine learning methods trained on
past GSR-alert matches). This expected quality score measure provided a way for analysts to use quality
directly as a way to tune the system to receive greater or fewer
alerts.  Figure~\ref{fig:recallVsQS} shows the trade-off between final quality
score and recall when alerts are suppressed based on expected quality.
As expected we can see that the recall drops and quality increases as the cut-off
threshold for expected quality is increased.

\narenc{Show forecast with comment describing predicted QS.}
\begin{figure}[t]
\includegraphics[width=\columnwidth]{figures/cu/perf_figures.pdf}
\caption{Comparison of number of perfect scores(4.0) obtained by EMBERS vs BaseRate per month in 2013.}
\label{fig:perfect_score}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{figures/cu/recallVsQS.pdf}
%\includegraphics[height=0.2\textheight]{figures/cu/recallVsQS.pdf}
\caption{Recall vs Quality Trade-off}
\label{fig:recallVsQS}
\end{figure}

\input{figures/cu/narrative.tex}
\narenc{Write explanation of how narrative (Figure~\ref{fig:narrative}) was come up with.}
